#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Deep Q-Learning for Lunar Lander
æœ¬é¡¹ç›®å®ç°äº†å¸¦æœ‰ç»éªŒå›æ”¾çš„Deep Q-Learningç®—æ³•ï¼Œç”¨äºè®­ç»ƒæ™ºèƒ½ä½“åœ¨LunarLander-v2ç¯å¢ƒä¸­å®‰å…¨ç€é™†
"""

import time
import numpy as np
import tensorflow as tf
from collections import deque, namedtuple
import gym
import matplotlib.pyplot as plt
import pickle
import os

# è®¾ç½®éšæœºç§å­
SEED = 42
tf.random.set_seed(SEED)
np.random.seed(SEED)

# è¶…å‚æ•°
MEMORY_SIZE = 100_000
GAMMA = 0.995
ALPHA = 1e-3
NUM_STEPS_FOR_UPDATE = 4
BATCH_SIZE = 64
EPSILON_START = 1.0
EPSILON_END = 0.01
EPSILON_DECAY = 0.995
TARGET_UPDATE_FREQ = 100

class MemoryBuffer:
    """ç»éªŒå›æ”¾ç¼“å†²åŒº"""
    
    def __init__(self, max_size):
        self.buffer = deque(maxlen=max_size)
    
    def add(self, experience):
        self.buffer.append(experience)
    
    def sample(self, batch_size=BATCH_SIZE):
        batch_size = min(batch_size, len(self.buffer))
        experiences = np.random.choice(self.buffer, batch_size, replace=False)
        return experiences
    
    def __len__(self):
        return len(self.buffer)

def create_q_network(state_size, num_actions):
    """åˆ›å»ºQç½‘ç»œ"""
    model = tf.keras.Sequential([
        tf.keras.layers.Input(shape=(state_size,)),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dense(num_actions, activation='linear')
    ])
    
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=ALPHA), loss='mse')
    return model

def update_target_network(q_network, target_q_network):
    """æ›´æ–°ç›®æ ‡ç½‘ç»œæƒé‡"""
    for target_weights, q_weights in zip(target_q_network.weights, q_network.weights):
        target_weights.assign(q_weights)

def get_action(q_values, epsilon):
    """ä½¿ç”¨Îµ-è´ªå©ªç­–ç•¥é€‰æ‹©åŠ¨ä½œ"""
    if np.random.random() < epsilon:
        # éšæœºæ¢ç´¢
        action = np.random.randint(0, q_values.shape[1])
    else:
        # è´ªå©ªé€‰æ‹©
        action = tf.argmax(q_values[0]).numpy()
    
    return action

def get_new_eps(epsilon):
    """æ›´æ–°Îµå€¼"""
    epsilon = max(EPSILON_END, epsilon * EPSILON_DECAY)
    return epsilon

def check_update_conditions(t, num_steps_upd, memory_buffer):
    """æ£€æŸ¥æ˜¯å¦åº”è¯¥æ›´æ–°ç½‘ç»œ"""
    if (t + 1) % num_steps_upd == 0 and len(memory_buffer) > 1000:
        return True
    return False

def get_experiences(memory_buffer):
    """ä»ç»éªŒå›æ”¾ç¼“å†²åŒºä¸­éšæœºé‡‡æ ·ç»éªŒ"""
    experiences = memory_buffer.sample()
    states = tf.cast([e[0] for e in experiences], tf.float32)
    actions = tf.cast([e[1] for e in experiences], tf.int32)
    rewards = tf.cast([e[2] for e in experiences], tf.float32)
    next_states = tf.cast([e[3] for e in experiences], tf.float32)
    done_vals = tf.cast([e[4] for e in experiences], tf.float32)
    
    return (states, actions, rewards, next_states, done_vals)

def compute_loss(experiences, gamma, q_network, target_q_network):
    """è®¡ç®—æŸå¤±å‡½æ•°"""
    states, actions, rewards, next_states, done_vals = experiences
    
    # è®¡ç®—ç›®æ ‡Qå€¼
    max_qsa = tf.reduce_max(target_q_network(next_states), axis=-1)
    y_targets = rewards + (1 - done_vals) * gamma * max_qsa
    
    # è®¡ç®—å½“å‰Qå€¼
    q_values = q_network(states)
    q_values = tf.gather_nd(q_values, tf.stack([tf.range(q_values.shape[0]), actions], axis=1))
    
    # è®¡ç®—MSEæŸå¤±
    loss = tf.keras.losses.MSE(y_targets, q_values)
    return loss

@tf.function
def agent_learn(experiences, gamma, q_network, target_q_network, optimizer):
    """æ™ºèƒ½ä½“å­¦ä¹ å‡½æ•°"""
    with tf.GradientTape() as tape:
        loss = compute_loss(experiences, gamma, q_network, target_q_network)
    
    # è®¡ç®—æ¢¯åº¦å¹¶åº”ç”¨
    gradients = tape.gradient(loss, q_network.trainable_variables)
    optimizer.apply_gradients(zip(gradients, q_network.trainable_variables))
    
    return loss

def plot_history(total_point_history, num_p_av=100):
    """ç»˜åˆ¶è®­ç»ƒå†å²"""
    # è®¡ç®—ç§»åŠ¨å¹³å‡
    av_latest_points = []
    for idx in range(num_p_av, len(total_point_history) + 1):
        av_latest_points.append(np.mean(total_point_history[idx - num_p_av:idx]))
    
    # åˆ›å»ºå›¾å½¢
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
    
    # ç»˜åˆ¶æ€»ç‚¹æ•°å†å²
    ax1.plot(total_point_history)
    ax1.set_title('è®­ç»ƒå¥–åŠ±å†å²')
    ax1.set_xlabel('Episode')
    ax1.set_ylabel('æ€»å¥–åŠ±')
    ax1.grid(True)
    
    # ç»˜åˆ¶ç§»åŠ¨å¹³å‡
    ax2.plot(av_latest_points)
    ax2.set_title(f'æœ€å{num_p_av}ä¸ªepisodesçš„å¹³å‡å¥–åŠ±')
    ax2.set_xlabel('Episode')
    ax2.set_ylabel('å¹³å‡å¥–åŠ±')
    ax2.grid(True)
    
    plt.tight_layout()
    plt.show()

def evaluate_agent(env, q_network, num_episodes=10):
    """è¯„ä¼°æ™ºèƒ½ä½“æ€§èƒ½"""
    total_rewards = []
    
    for episode in range(num_episodes):
        state = env.reset()
        total_reward = 0
        done = False
        
        while not done:
            state_qn = np.expand_dims(state, axis=0)
            q_values = q_network(state_qn)
            action = get_action(q_values, 0.0)  # ä½¿ç”¨è´ªå©ªç­–ç•¥
            
            state, reward, done, info = env.step(action)
            total_reward += reward
        
        total_rewards.append(total_reward)
    
    env.close()
    
    avg_reward = np.mean(total_rewards)
    std_reward = np.std(total_rewards)
    
    print(f"è¯„ä¼°ç»“æœ ({num_episodes} episodes):")
    print(f"å¹³å‡å¥–åŠ±: {avg_reward:.2f} Â± {std_reward:.2f}")
    print(f"æœ€å°å¥–åŠ±: {np.min(total_rewards):.2f}")
    print(f"æœ€å¤§å¥–åŠ±: {np.max(total_rewards):.2f}")
    
    return avg_reward, std_reward

def train_agent(env, q_network, target_q_network, memory_buffer, num_episodes=2000, max_steps_per_episode=1000):
    """è®­ç»ƒæ™ºèƒ½ä½“"""
    
    total_point_history = []
    num_p_av = 100
    epsilon = EPSILON_START
    
    print(f"å¼€å§‹è®­ç»ƒï¼Œç›®æ ‡episodes: {num_episodes}")
    print(f"è§£å†³é˜ˆå€¼: 200 (å¹³å‡å¥–åŠ±è¶…è¿‡{num_p_av}ä¸ªepisodes)")
    
    # è®¾ç½®ç›®æ ‡ç½‘ç»œæƒé‡ç­‰äºQç½‘ç»œæƒé‡
    target_q_network.set_weights(q_network.get_weights())
    
    for episode in range(num_episodes):
        # é‡ç½®ç¯å¢ƒ
        state = env.reset()
        total_points = 0
        
        for step in range(max_steps_per_episode):
            # é€‰æ‹©åŠ¨ä½œ
            state_qn = np.expand_dims(state, axis=0)
            q_values = q_network(state_qn)
            action = get_action(q_values, epsilon)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, done, info = env.step(action)
            
            # å­˜å‚¨ç»éªŒ
            experience = (state, action, reward, next_state, done)
            memory_buffer.add(experience)
            
            # æ£€æŸ¥æ˜¯å¦åº”è¯¥æ›´æ–°ç½‘ç»œ
            update = check_update_conditions(step, NUM_STEPS_FOR_UPDATE, memory_buffer)
            
            if update:
                # é‡‡æ ·ç»éªŒå¹¶è®­ç»ƒ
                experiences = get_experiences(memory_buffer)
                agent_learn(experiences, GAMMA, q_network, target_q_network, q_network.optimizer)
            
            state = next_state.copy()
            total_points += reward
            
            if done:
                break
        
        # æ›´æ–°ç›®æ ‡ç½‘ç»œ
        if episode % TARGET_UPDATE_FREQ == 0:
            update_target_network(q_network, target_q_network)
        
        # è¡°å‡æ¢ç´¢ç‡
        epsilon = get_new_eps(epsilon)
        
        # è®°å½•å†å²
        total_point_history.append(total_points)
        
        # æ‰“å°è¿›åº¦
        if episode % 100 == 0:
            av_latest_points = np.mean(total_point_history[-num_p_av:])
            print(f"Episode {episode:4d} | å¥–åŠ±: {total_points:6.1f} | å¹³å‡å¥–åŠ±: {av_latest_points:6.1f} | Epsilon: {epsilon:.3f}")
        
        # æ£€æŸ¥æ˜¯å¦è§£å†³ç¯å¢ƒ
        if len(total_point_history) >= num_p_av:
            recent_avg = np.mean(total_point_history[-num_p_av:])
            if recent_avg >= 200.0:
                print(f"\nğŸ‰ ç¯å¢ƒå·²è§£å†³ï¼Episode {episode} æ—¶å¹³å‡å¥–åŠ±è¾¾åˆ° {recent_avg:.1f}")
                break
    
    print(f"\nè®­ç»ƒå®Œæˆï¼æ€»å…±è®­ç»ƒäº† {len(total_point_history)} ä¸ªepisodes")
    
    return total_point_history

def main():
    """ä¸»å‡½æ•°"""
    print("Deep Q-Learning for Lunar Lander")
    print("=" * 50)
    
    # åˆ›å»ºç¯å¢ƒ
    env = gym.make('LunarLander-v2')
    state_size = env.observation_space.shape[0]
    num_actions = env.action_space.n
    
    print(f"ç¯å¢ƒ: {env.unwrapped.spec.id}")
    print(f"çŠ¶æ€ç©ºé—´ç»´åº¦: {state_size}")
    print(f"åŠ¨ä½œç©ºé—´å¤§å°: {num_actions}")
    
    # åˆ›å»ºç½‘ç»œ
    q_network = create_q_network(state_size, num_actions)
    target_q_network = create_q_network(state_size, num_actions)
    
    print(f"\nQç½‘ç»œå‚æ•°æ•°é‡: {q_network.count_params():,}")
    print("å¼€å§‹è®­ç»ƒ...")
    
    # åˆ›å»ºç»éªŒå›æ”¾ç¼“å†²åŒº
    memory_buffer = MemoryBuffer(MEMORY_SIZE)
    
    # è®­ç»ƒæ™ºèƒ½ä½“
    start_time = time.time()
    total_point_history = train_agent(env, q_network, target_q_network, memory_buffer, num_episodes=2000)
    training_time = time.time() - start_time
    
    print(f"\nè®­ç»ƒå®Œæˆï¼æ€»ç”¨æ—¶: {training_time/60:.1f} åˆ†é’Ÿ")
    
    # ç»˜åˆ¶ç»“æœ
    plot_history(total_point_history)
    
    # è¯„ä¼°æ™ºèƒ½ä½“
    print("\nè¯„ä¼°è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“...")
    avg_reward, std_reward = evaluate_agent(env, q_network, num_episodes=10)
    
    # ä¿å­˜æ¨¡å‹
    model_filename = 'lunar_lander_dql_model.h5'
    q_network.save(model_filename)
    print(f"\næ¨¡å‹å·²ä¿å­˜ä¸º: {model_filename}")
    
    # ä¿å­˜è®­ç»ƒå†å²
    history_filename = 'training_history.pkl'
    with open(history_filename, 'wb') as f:
        pickle.dump({
            'total_point_history': total_point_history
        }, f)
    print(f"è®­ç»ƒå†å²å·²ä¿å­˜ä¸º: {history_filename}")
    
    # è®¡ç®—æœ€ç»ˆæ€§èƒ½
    final_avg_reward = np.mean(total_point_history[-100:])
    final_std_reward = np.std(total_point_history[-100:])
    
    print(f"\næœ€ç»ˆæ€§èƒ½ (æœ€å100ä¸ªepisodes):")
    print(f"å¹³å‡å¥–åŠ±: {final_avg_reward:.2f} Â± {final_std_reward:.2f}")
    print(f"æœ€å¤§å¥–åŠ±: {np.max(total_point_history):.2f}")
    print(f"æœ€å°å¥–åŠ±: {np.min(total_point_history):.2f}")
    
    env.close()
    print("\nğŸ‰ é¡¹ç›®å®Œæˆï¼")

if __name__ == "__main__":
    main()
